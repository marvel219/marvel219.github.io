<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>网络爬虫/相关工具与知识</title>
    <link href="/2022/05/30/newpaper/"/>
    <url>/2022/05/30/newpaper/</url>
    
    <content type="html"><![CDATA[<h2 id="网络爬虫"><a href="#网络爬虫" class="headerlink" title="网络爬虫"></a>网络爬虫</h2><p>网络爬虫(web crawler), 以前经常称为网络蜘蛛(spider), 是按照一定的规则自动浏览万维网并获取信息的机器人程序(或叫脚本), 曾经被广泛的应用于互联网搜索引擎.<br>使用过互联网和浏览器的人都知道, 网页中除了提供用户阅读的文字信息之外, 还包含一些超链接. 网络爬虫系统正是通过网页中的超链接信息不断获得网络上的其他页面. 正因为如此, 网络数据采集的过程就像一个爬虫或者蜘蛛在网络上漫游, 所有才被形象的称之为网络爬虫或者网络蜘蛛.</p><h3 id="爬虫的应用领域"><a href="#爬虫的应用领域" class="headerlink" title="爬虫的应用领域"></a>爬虫的应用领域</h3><p>在理想的状态下, 所有的ICP(internet Content Provider) 都应该为自己的网络提供API接口来共享它们允许其他程序获取的数据, 在这种情况下爬虫就不是必需品, 国内比较有名的电商平台(如淘宝, 京东等), 社交平台(如QQ&#x2F;微博&#x2F;微信等)这些网站都提供了自己的Open Api, 但是这类Open Api通常会对可以抓取的数据频率进行限制. 对于大多数的公司而言, 计时的获取行业相关数据就是企业生存的重要环节之一, 然而大部分企业在行业数据方面的匮乏是其与生俱来的短板, 合理的利用爬虫来获取数据并从中提取出有价值的信息是至关重要的. 当然爬虫还有很多重要的应用灵玉, 以下列举了其中一部分.</p><figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">1. 搜索引擎<br>2. 新闻聚合<br>3. 社交应用<br>4. 舆情监控<br>5. 行业数据<br></code></pre></div></td></tr></table></figure><h3 id="Rbots-txt文件"><a href="#Rbots-txt文件" class="headerlink" title="Rbots.txt文件"></a>Rbots.txt文件</h3><p>大多数网站都会定义robots.txt文件, 下面以淘宝的robots.txt文件为例, 看看该网站对爬虫有哪些限制</p><figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-variable">$User</span>-agent:  Baiduspider<br>Allow:  /article<br>Allow:  /oshtml<br>Disallow:  /product/<br>Disallow:  /<br><br>User-Agent:  Googlebot<br>Allow:  /article<br>Allow:  /oshtml<br>Allow:  /product<br>Allow:  /spu<br>Allow:  /dianpu<br>Allow:  /oversea<br>Allow:  /list<br>Disallow:  /<br><br>User-agent:  Bingbot<br>Allow:  /article<br>Allow:  /oshtml<br>Allow:  /product<br>Allow:  /spu<br>Allow:  /dianpu<br>Allow:  /oversea<br>Allow:  /list<br>Disallow:  /<br><br>User-Agent:  360Spider<br>Allow:  /article<br>Allow:  /oshtml<br>Disallow:  /<br><br>User-Agent:  Yisouspider<br>Allow:  /article<br>Allow:  /oshtml<br>Disallow:  /<br><br>User-Agent:  Sogouspider<br>Allow:  /article<br>Allow:  /oshtml<br>Allow:  /product<br>Disallow:  /<br><br>User-Agent:  Yahoo!  Slurp<br>Allow:  /product<br>Allow:  /spu<br>Allow:  /dianpu<br>Allow:  /oversea<br>Allow:  /list<br>Disallow:  /<br><br>User-Agent:  *<br>Disallow:  /<br></code></pre></div></td></tr></table></figure><p>注意上面robots.txt第一段的最后一行, 通过设置’Disallow:&#x2F;’禁止百度爬虫访问除了’Allow’规定页面外的其他所有页面. 因此当你在百度搜索’淘宝’的时候, 搜索结果下方会出现: ‘由于该网站的rebots.txt文件存在限制指令(限制搜索引擎抓取). 系统无法提供该页面的内容描述.’, 百度作为一个搜索引擎, 至少在表面上遵守了淘宝网的robots.txt协议, 所以用户不能从百度上搜索到淘宝内部的产品信息.</p><h2 id="相关工具介绍"><a href="#相关工具介绍" class="headerlink" title="相关工具介绍"></a>相关工具介绍</h2><p>在开始讲解爬虫之前，我们稍微对HTTP（超文本传输协议）做一些回顾，因为我们在网页上看到的内容通常是浏览器执行HTML语言得到的结果，而HTTP就是传输HTML数据的协议。HTTP是构建于TCP（传输控制协议）之上应用级协议，它利用了TCP提供的可靠的传输服务实现了Web应用中的数据交换。按照维基百科上的介绍，设计HTTP最初的目的是为了提供一种发布和接收HTML页面的方法，也就是说这个协议是浏览器和Web服务器之间传输的数据的载体。关于这个协议的详细信息以及目前的发展状况，大家可以阅读阮一峰老师的<a href="http://www.ruanyifeng.com/blog/2016/08/http.html">《HTTP 协议入门》</a>、<a href="http://www.ruanyifeng.com/blog/2012/05/internet_protocol_suite_part_i.html">《互联网协议入门》</a>系列以及<a href="http://www.ruanyifeng.com/blog/2014/09/illustration-ssl.html">《图解HTTPS协议》</a>进行了解，下图是我在2009年9月10日凌晨4点在四川省网络通信技术重点实验室用开源协议分析工具Ethereal（抓包工具WireShark的前身）截取的访问百度首页时的HTTP请求和响应的报文（协议数据），由于Ethereal截取的是经过网络适配器的数据，因此可以清晰的看到从物理链路层到应用层的协议数据。</p><h3 id="相关工具"><a href="#相关工具" class="headerlink" title="相关工具"></a>相关工具</h3><p>1.Chrome Developer Tools</p><p>开发推荐使用谷歌浏览器, 这是谷歌的开发者工具</p><p>2.HTTPie</p><figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ http --header http://www.scu.edu.cn<br>HTTP/1.1 200 OK<br>Accept-Ranges: bytes<br>Cache-Control: private, max-age=600<br>Connection: Keep-Alive<br>Content-Encoding: gzip<br>Content-Language: zh-CN<br>Content-Length: 14403<br>Content-Type: text/html<br>Date: Sun, 27 May 2018 15:38:25 GMT<br>ETag: <span class="hljs-string">&quot;e6ec-56d3032d70a32-gzip&quot;</span><br>Expires: Sun, 27 May 2018 15:48:25 GMT<br>Keep-Alive: <span class="hljs-built_in">timeout</span>=5, max=100<br>Last-Modified: Sun, 27 May 2018 13:44:22 GMT<br>Server: VWebServer<br>Vary: User-Agent,Accept-Encoding<br>X-Frame-Options: SAMEORIGIN<br></code></pre></div></td></tr></table></figure><p>3.BuiltWith(python自带的模块): 识别网站使用的技术</p><figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">&gt;&gt;&gt;<br>&gt;&gt;&gt; import builtwith<br>&gt;&gt;&gt; builtwith.parse(<span class="hljs-string">&#x27;http://www.bootcss.com/&#x27;</span>)<br>&#123;<span class="hljs-string">&#x27;web-servers&#x27;</span>: [<span class="hljs-string">&#x27;Nginx&#x27;</span>], <span class="hljs-string">&#x27;font-scripts&#x27;</span>: [<span class="hljs-string">&#x27;Font Awesome&#x27;</span>], <span class="hljs-string">&#x27;javascript-frameworks&#x27;</span>: [<span class="hljs-string">&#x27;Lo-dash&#x27;</span>, <span class="hljs-string">&#x27;Underscore.js&#x27;</span>, <span class="hljs-string">&#x27;Vue.js&#x27;</span>, <span class="hljs-string">&#x27;Zepto&#x27;</span>, <span class="hljs-string">&#x27;jQuery&#x27;</span>], <span class="hljs-string">&#x27;web-frameworks&#x27;</span>: [<span class="hljs-string">&#x27;Twitter Bootstrap&#x27;</span>]&#125;<br><br>&gt;&gt;&gt;<br>&gt;&gt;&gt; import ssl<br>&gt;&gt;&gt; ssl._create_default_https_context = ssl._create_unverified_context<br>&gt;&gt;&gt; builtwith.parse(<span class="hljs-string">&#x27;https://www.jianshu.com/&#x27;</span>)<br>&#123;<span class="hljs-string">&#x27;web-servers&#x27;</span>: [<span class="hljs-string">&#x27;Tengine&#x27;</span>], <span class="hljs-string">&#x27;web-frameworks&#x27;</span>: [<span class="hljs-string">&#x27;Twitter Bootstrap&#x27;</span>, <span class="hljs-string">&#x27;Ruby on Rails&#x27;</span>], <span class="hljs-string">&#x27;programming-languages&#x27;</span>: [<span class="hljs-string">&#x27;Ruby&#x27;</span>]&#125;<br></code></pre></div></td></tr></table></figure><p>4.robotparser: 解析robots.txt的工具</p><figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">&gt;&gt;&gt; from urllib import robotparser<br>&gt;&gt;&gt; parser = robotparser.RobotFileParser()<br>&gt;&gt;&gt; parser.set_url(<span class="hljs-string">&#x27;https://www.taobao.com/robots.txt&#x27;</span>)<br>&gt;&gt;&gt; parser.read()<br>&gt;&gt;&gt; parser.can_fetch(<span class="hljs-string">&#x27;Hellokitty&#x27;</span>, <span class="hljs-string">&#x27;http://www.taobao.com/article&#x27;</span>)<br>False<br>&gt;&gt;&gt; parser.can_fetch(<span class="hljs-string">&#x27;Baiduspider&#x27;</span>, <span class="hljs-string">&#x27;http://www.taobao.com/article&#x27;</span>)<br>True<br>&gt;&gt;&gt; parser.can_fetch(<span class="hljs-string">&#x27;Baiduspider&#x27;</span>, <span class="hljs-string">&#x27;http://www.taobao.com/product&#x27;</span>)<br>False<br></code></pre></div></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>网络爬虫</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2022/05/07/hello-world/"/>
    <url>/2022/05/07/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></div></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ hexo server<br></code></pre></div></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ hexo generate<br></code></pre></div></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></div></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>test</tag>
      
      <tag>hello world</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
